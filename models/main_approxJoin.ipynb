{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b93311c-8c62-4668-bb72-a16edbc70cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a54fafaa-62c7-4930-9ea3-111b748001af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, DateType, BinaryType, BooleanType, ByteType, MapType\n",
    "from pyspark.sql.functions import expr, lit\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6fb96f9f-e86e-4281-bc72-be9eac92e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc01e6cf-933b-47e9-a979-0cf425d923fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Entity-Resolution-Matching').getOrCreate()\n",
    "sparkContext=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62694b47-e05a-457c-bfdc-7238c90bf1b1",
   "metadata": {},
   "source": [
    "## Master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22202809-9e15-4586-a679-f087a8f7d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",True).csv(\"hashed_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc1a48-c3a2-48c5-a1a4-988eea3a9ba5",
   "metadata": {},
   "source": [
    "## Candidates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5d5a3f3-cbc5-4756-8ded-6e5479b69f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trivial matching on the same data\n",
    "df2 = spark.read.option(\"header\", True).csv(\"hashed_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3340972c-6a64-4def-8875-4ca84e169b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(person_id='1000', service_date='2020-10-20', first_name='vicky', middle_name='Kathy', last_name='Singh', dob='1984-04-01', ssn='895-37-4654', name_phonetic='FK', ssn_clean='895-37-4654', name_phonetic_hash='[B@34d548a', ssn_clean_hash='[B@43a4256f'),\n",
       " Row(person_id='1001', service_date='2021-09-06', first_name='Gary', middle_name='Stacey', last_name='Watson', dob='1984-10-22', ssn='220-72-9331', name_phonetic='KR', ssn_clean='220-72-9331', name_phonetic_hash='[B@2166b6b2', ssn_clean_hash='[B@4d63d36b'),\n",
       " Row(person_id='1002', service_date='2020-11-07', first_name='Anna', middle_name='Joseph', last_name='Camacho', dob='1996-04-11', ssn='404-44-6841', name_phonetic='AN', ssn_clean='404-44-6841', name_phonetic_hash='[B@48e64f10', ssn_clean_hash='[B@495c4e1f')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04981bb-420e-4c40-95a5-fc5fa3f0fb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb1388db-ffda-4cd7-a283-10ae69494193",
   "metadata": {},
   "source": [
    "## Parameter Alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e75a732-c4ba-4e79-bd3c-5460dc411c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, import set of unique identifiers and fuzzy identifiers that are agreed upon in the matching\n",
    "f = open(\"param_config.json\")\n",
    "fields = json.load(f)\n",
    "# fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd20cffa-59fa-47e0-84be-d9feb2037edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_fields = [x['field_name']+'_hash' for x in fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b668ee40-1e47-48fd-9674-bd2d13180e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name_phonetic_hash', 'ssn_clean_hash']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551bda67-7045-421b-b313-fac3b8a8ff79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f97478ba-bf89-4e81-ad32-6fe23920261e",
   "metadata": {},
   "source": [
    "### Match new records df2 to existing records df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcebd82-1e17-4ae3-92cf-8fe32e6e7abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54e09fcc-595f-474d-b51c-213ebfeca71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(stages=[\n",
    "    RegexTokenizer(\n",
    "        pattern=\"\", inputCol=\"text\", outputCol=\"tokens\", minTokenLength=1\n",
    "    ),\n",
    "    NGram(n=3, inputCol=\"tokens\", outputCol=\"ngrams\"),\n",
    "    HashingTF(inputCol=\"ngrams\", outputCol=\"vectors\"),\n",
    "    MinHashLSH(inputCol=\"vectors\", outputCol=\"lsh\")\n",
    "]).fit(df)\n",
    "\n",
    "# db_hashed = model.transform(db)\n",
    "# query_hashed = model.transform(query)\n",
    "\n",
    "# model.stages[-1].approxSimilarityJoin(db_hashed, query_hashed, 0.75).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b6f3c21-f94d-4580-a288-bd123b602375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f04dd946-9550-4fd3-bb0f-203ad314495c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---------+------------+----------+-----------+---------+----------+-----------+-------------+---------+------------------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|person_id|service_date|first_name|middle_name|last_name|       dob|        ssn|name_phonetic|ssn_clean|name_phonetic_hash|ssn_clean_hash|   features|              tokens|              ngrams|name_phonetic_hash_2|                 lsh|\n",
      "+---------+------------+----------+-----------+---------+----------+-----------+-------------+---------+------------------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "|     1000|  2020-10-20|     vicky|      Kathy|    Singh|1984-04-01|895-37-4654|           10|       10|        [B@34d548a|   [B@43a4256f|[10.0,10.0]|[[, b, @, 3, 4, d...|[[ b @, b @ 3, @ ...|(262144,[23995,44...|[[4.90017082E8], ...|\n",
      "|     1001|  2021-09-06|      Gary|     Stacey|   Watson|1984-10-22|220-72-9331|           10|       10|       [B@2166b6b2|   [B@4d63d36b|[10.0,10.0]|[[, b, @, 2, 1, 6...|[[ b @, b @ 2, @ ...|(262144,[10120,23...|[[3.37850741E8], ...|\n",
      "+---------+------------+----------+-----------+---------+----------+-----------+-------------+---------+------------------+--------------+-----------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# brp = MinHashLSH(inputCol='name_phonetic_hash', outputCol=\"lsh\", numHashTables=3)\n",
    "\n",
    "# model = brp.fit(df_feat)\n",
    "\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(df_feat).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "340066ec-5a15-407d-af40-1d13c3f2b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_hashed = model.transform(df_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b63c2d8e-bc1e-42da-8b1a-77e686b050df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-22 10:29:58 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+\n",
      "|            datasetA|            datasetB|           distCol|\n",
      "+--------------------+--------------------+------------------+\n",
      "|[1019, 2021-01-27...|[1019, 2021-01-27...|               0.0|\n",
      "|[1009, 2021-01-06...|[1009, 2021-01-06...|               0.0|\n",
      "|[1003, 2021-10-24...|[1003, 2021-10-24...|               0.0|\n",
      "|[1000, 2020-10-20...|[1000, 2020-10-20...|               0.0|\n",
      "|[1015, 2021-02-17...|[1015, 2021-02-17...|               0.0|\n",
      "|[1024, 2021-04-21...|[1024, 2021-04-21...|               0.0|\n",
      "|[1012, 2021-07-07...|[1012, 2021-07-07...|               0.0|\n",
      "|[1007, 2021-10-21...|[1007, 2021-10-21...|               0.0|\n",
      "|[1006, 2020-11-07...|[1006, 2020-11-07...|               0.0|\n",
      "|[1022, 2021-08-17...|[1022, 2021-08-17...|               0.0|\n",
      "|[1001, 2021-09-06...|[1001, 2021-09-06...|               0.0|\n",
      "|[1005, 2021-08-10...|[1005, 2021-08-10...|               0.0|\n",
      "|[1010, 2020-09-20...|[1010, 2020-09-20...|               0.0|\n",
      "|[1006, 2020-11-07...|[1010, 2020-09-20...|0.7142857142857143|\n",
      "|[1017, 2021-10-27...|[1017, 2021-10-27...|               0.0|\n",
      "|[1008, 2021-05-09...|[1008, 2021-05-09...|               0.0|\n",
      "|[1020, 2021-05-28...|[1020, 2021-05-28...|               0.0|\n",
      "|[1004, 2020-12-26...|[1004, 2020-12-26...|               0.0|\n",
      "|[1018, 2021-01-27...|[1018, 2021-01-27...|               0.0|\n",
      "|[1023, 2021-10-16...|[1023, 2021-10-16...|               0.0|\n",
      "+--------------------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.stages[-1].approxSimilarityJoin(db_hashed, db_hashed, 0.75).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9eed50ab-b612-4eee-9be1-c770df5348c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately joining dfA and dfB on Jaccard coefficient distance smaller than 1.5:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PipelineModel' object has no attribute 'approxSimilarityJoin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ns/ynfg301x3n10ddvyqgxx1rn40000gn/T/ipykernel_63453/2665061973.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Approximately joining dfA and dfB on Jaccard coefficient distance smaller than 1.5:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapproxSimilarityJoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_feat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Jaccard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     .select(col(\"datasetA.person_id\").alias(\"idA\"),\n\u001b[1;32m      8\u001b[0m             \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasetB.person_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"idA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PipelineModel' object has no attribute 'approxSimilarityJoin'"
     ]
    }
   ],
   "source": [
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "print(\"Approximately joining dfA and dfB on Jaccard coefficient distance smaller than 1.5:\")\n",
    "model.approxSimilarityJoin(df_feat, df_feat, 1.5, distCol=\"Jaccard\")\\\n",
    "    .select(col(\"datasetA.person_id\").alias(\"idA\"),\n",
    "            col(\"datasetB.person_id\").alias(\"idA\"),\n",
    "            col(\"Jaccard\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072e9e9-ee52-4095-8891-aed7d7407e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1da8f407-f047-415c-ab84-294d92685968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff728cfa-55a4-42e1-9944-f24b5eb58f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06205f37-4ced-401d-b21d-fe8325f623d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56ee1d77-032e-4c4c-9d9f-5943c425d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our hashes, we need to decide on a similarity distance and a approximate matching algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba2b76-69e3-44a4-9e01-db795c78a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 1: home made dice coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96d0575d-f3b4-4e16-af92-83c51fece150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_digits(bits):\n",
    "    digits = [int(x) for x in list(bits)]\n",
    "    return sum(digits)\n",
    "\n",
    "def dice_coeff(bits1, bits2):\n",
    "    return '10'\n",
    "#     bits = list(zip(bits1, bits2))\n",
    "#     commons = sum([x[0] == x[1] for x in bits])\n",
    "#     return (2 * commons) / (sum_digits(bits1) + sum_digits(bits2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "630a5ddb-283c-4388-832e-cea034f0adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_dice = udf(dice_coeff, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "844136c9-cf09-4901-9653-040a37923e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df1.alias(\"left\")\n",
    "right = df2.alias(\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "494445f7-34f4-4842-a3b9-d660efba7924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(person_id='1000', dx='10'),\n",
       " Row(person_id='1000', dx='10'),\n",
       " Row(person_id='1000', dx='10'),\n",
       " Row(person_id='1000', dx='10'),\n",
       " Row(person_id='1000', dx='10')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.crossJoin(right).select(df1.person_id,\n",
    "      udf_dice(left.bloom_1, right.bloom_1).alias(\"dx\")).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f9616-e4b1-475c-884c-f282c5fd33a1",
   "metadata": {},
   "source": [
    "self join code for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3ff381-278c-4853-ae87-34b008a19ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = sparkContext.parallelize(\n",
    "    [(\"a\", 1,2),(\"a\",1,4),(\"b\",5,6),(\"b\",10,2),(\"c\",1,1)]\n",
    "  ).toDF()#\"id\",\"x\",\"y\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4aacb4-1e62-4529-bca6-f756aaa8dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_1='a', _2=1, _3=2),\n",
       " Row(_1='a', _2=1, _3=4),\n",
       " Row(_1='b', _2=5, _3=6),\n",
       " Row(_1='b', _2=10, _3=2),\n",
       " Row(_1='c', _2=1, _3=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8db6ea9-0b64-4a62-baa2-0b32a7c4e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_1='c', dx=0, dy=0),\n",
       " Row(_1='b', dx=0, dy=0),\n",
       " Row(_1='b', dx=0, dy=0),\n",
       " Row(_1='b', dx=0, dy=0),\n",
       " Row(_1='b', dx=0, dy=0),\n",
       " Row(_1='a', dx=0, dy=0),\n",
       " Row(_1='a', dx=0, dy=0),\n",
       " Row(_1='a', dx=0, dy=0),\n",
       " Row(_1='a', dx=0, dy=0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = df.alias(\"left\")\n",
    "right = df.alias(\"right\")\n",
    "\n",
    "left.join(right,\"_1\").select(df._1,\n",
    "      (left._2-right._2).alias(\"dx\"),\n",
    "      (left._3-right._3).alias(\"dy\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ca01f-8c8c-4f55-94bc-aa27571c9f80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6233abbe-b6f2-43a6-afae-3d2c7ed04329",
   "metadata": {},
   "source": [
    "## Approx LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20e605ba-fe05-4141-ad70-3aa13ad77a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataA = [(0, Vectors.dense([1.0, 1.0]),),\n",
    "         (1, Vectors.dense([1.0, -1.0]),),\n",
    "         (2, Vectors.dense([-1.0, -1.0]),),\n",
    "         (3, Vectors.dense([-1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "\n",
    "dataB = [(4, Vectors.dense([1.0, 0.0]),),\n",
    "         (5, Vectors.dense([-1.0, 0.0]),),\n",
    "         (6, Vectors.dense([0.0, 1.0]),),\n",
    "         (7, Vectors.dense([0.0, -1.0]),)]\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n",
    "\n",
    "key = Vectors.dense([1.0, 0.0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82b7a9ca-8835-4b87-8d48-196e580b8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---+-----------+--------------------+\n",
      "| id|   features|              hashes|\n",
      "+---+-----------+--------------------+\n",
      "|  0|  [1.0,1.0]|[[6.70322104E8], ...|\n",
      "|  1| [1.0,-1.0]|[[6.70322104E8], ...|\n",
      "|  2|[-1.0,-1.0]|[[6.70322104E8], ...|\n",
      "|  3| [-1.0,1.0]|[[6.70322104E8], ...|\n",
      "+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brp = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\",\n",
    "                                  numHashTables=3)\n",
    "model = brp.fit(dfA)\n",
    "\n",
    "brp.fit(dfB)\n",
    "\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "347ba93a-8d1e-423e-ba00-db187a1ad1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|  features|              hashes|\n",
      "+---+----------+--------------------+\n",
      "|  4| [1.0,0.0]|[[6.70322104E8], ...|\n",
      "|  5|[-1.0,0.0]|[[6.70322104E8], ...|\n",
      "|  6| [0.0,1.0]|[[1.055109162E9],...|\n",
      "|  7|[0.0,-1.0]|[[1.055109162E9],...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(dfB).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc48a7df-0540-4e0f-b5ef-921ee0c2713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately joining dfA and dfB on Jaccard coefficient distance smaller than 1.5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|idA|idB|Jaccard|\n",
      "+---+---+-------+\n",
      "|  2|  6|    0.5|\n",
      "|  3|  6|    0.5|\n",
      "|  0|  5|    0.5|\n",
      "|  1|  7|    0.5|\n",
      "|  0|  4|    0.5|\n",
      "|  2|  7|    0.5|\n",
      "|  1|  5|    0.5|\n",
      "|  2|  5|    0.5|\n",
      "|  0|  7|    0.5|\n",
      "|  1|  4|    0.5|\n",
      "|  0|  6|    0.5|\n",
      "|  2|  4|    0.5|\n",
      "|  3|  7|    0.5|\n",
      "|  1|  6|    0.5|\n",
      "|  3|  4|    0.5|\n",
      "|  3|  5|    0.5|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "print(\"Approximately joining dfA and dfB on Jaccard coefficient distance smaller than 1.5:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 1.5, distCol=\"Jaccard\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"Jaccard\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ce6b313-50ff-4063-8553-68e1e5a5f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately searching dfA for 2 nearest neighbors of the key:\n",
      "+---+----------+--------------------+-------+\n",
      "| id|  features|              hashes|distCol|\n",
      "+---+----------+--------------------+-------+\n",
      "|  0| [1.0,1.0]|[[6.70322104E8], ...|    0.5|\n",
      "|  1|[1.0,-1.0]|[[6.70322104E8], ...|    0.5|\n",
      "+---+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "# neighbor search.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "print(\"Approximately searching dfA for 2 nearest neighbors of the key:\")\n",
    "candidates = model.approxNearestNeighbors(dfA, key, 2)\n",
    "candidates.filter(candidates.distCol > 0.4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2bfb4-2395-42c2-87de-84b498c4ca9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
