{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b93311c-8c62-4668-bb72-a16edbc70cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54fafaa-62c7-4930-9ea3-111b748001af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, DateType, BinaryType, BooleanType, ByteType, MapType\n",
    "from pyspark.sql.functions import expr, lit\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb96f9f-e86e-4281-bc72-be9eac92e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import RegexTokenizer, NGram, HashingTF, MinHashLSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc01e6cf-933b-47e9-a979-0cf425d923fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-25 13:37:51 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-25 13:37:52 WARN  Utils:66 - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Entity-Resolution-Matching').getOrCreate()\n",
    "sparkContext=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62694b47-e05a-457c-bfdc-7238c90bf1b1",
   "metadata": {},
   "source": [
    "## Master data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25405a49-53bc-4c49-a3f4-a536b95684b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is going to be input from CLI / json\n",
    "data_id = 'hashed_output_bloomhash_1650943904'\n",
    "key = '_bloomhash'\n",
    "timestamp = '1650943904'\n",
    "dedup = True\n",
    "if not dedup:\n",
    "    data_id_2 = 'SPECIFY'\n",
    "else:\n",
    "    data_id_2 = data_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22202809-9e15-4586-a679-f087a8f7d613",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\",True).csv(data_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc1a48-c3a2-48c5-a1a4-988eea3a9ba5",
   "metadata": {},
   "source": [
    "## Candidates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5d5a3f3-cbc5-4756-8ded-6e5479b69f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trivial matching on the same data\n",
    "df2 = spark.read.option(\"header\", True).csv(data_id_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3340972c-6a64-4def-8875-4ca84e169b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(person_id='1000', service_date='2020-10-20', first_name='vicky', middle_name='Kathy', last_name='Singh', dob='1984-04-01', ssn='895-37-4654', name_phonetic='FK', ssn_clean='895-37-4654', name_phonetic_bloomhash='[B@282985d2', ssn_clean_bloomhash='[B@5c5ea3f0'),\n",
       " Row(person_id='1001', service_date='2021-09-06', first_name='Gary', middle_name='Stacey', last_name='Watson', dob='1984-10-22', ssn='220-72-9331', name_phonetic='KR', ssn_clean='220-72-9331', name_phonetic_bloomhash='[B@4ff9867', ssn_clean_bloomhash='[B@5299460b'),\n",
       " Row(person_id='1002', service_date='2020-11-07', first_name='Anna', middle_name='Joseph', last_name='Camacho', dob='1996-04-11', ssn='404-44-6841', name_phonetic='AN', ssn_clean='404-44-6841', name_phonetic_bloomhash='[B@617aeb05', ssn_clean_bloomhash='[B@56a6c880')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1388db-ffda-4cd7-a283-10ae69494193",
   "metadata": {},
   "source": [
    "## Parameter Alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e75a732-c4ba-4e79-bd3c-5460dc411c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': [{'field_origin': 'first_name',\n",
       "   'field_name': 'name_phonetic',\n",
       "   'preprocess_steps': ['drop_chars', 'sort', 'phonetic'],\n",
       "   'matching_type': 'string',\n",
       "   'ngrams': True,\n",
       "   'weight': 0.5},\n",
       "  {'field_origin': 'ssn',\n",
       "   'field_name': 'ssn_clean',\n",
       "   'preprocess_steps': ['drop_chars'],\n",
       "   'matching_type': 'exact',\n",
       "   'ngrams': False,\n",
       "   'weight': 0.5}]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, import set of unique identifiers and fuzzy identifiers that are agreed upon in the matching\n",
    "f = open(timestamp + \"_param_config.json\")\n",
    "fields = json.load(f)\n",
    "# fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd20cffa-59fa-47e0-84be-d9feb2037edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_fields = [x['field_name']+ key for x in fields['fields']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b668ee40-1e47-48fd-9674-bd2d13180e99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name_phonetic_bloomhash', 'ssn_clean_bloomhash']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97478ba-bf89-4e81-ad32-6fe23920261e",
   "metadata": {},
   "source": [
    "### Match new records df2 to existing records df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcebd82-1e17-4ae3-92cf-8fe32e6e7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_hashes(data, fields_for_matching):\n",
    "    data.withColumn('long_hash', )\n",
    "    for col in matching_fields:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d24f75a9-3f0b-49fc-a898-c55e6f44f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "094ae68b-d90f-4c7d-8bfa-9d9c6bdd7725",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_left = df.select(concat(*matching_fields).alias(\"long_hash\"), 'person_id')  ##OOPS also add initial index\n",
    "df_right = df2.select(concat(*matching_fields).alias(\"long_hash\"), 'person_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54e09fcc-595f-474d-b51c-213ebfeca71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline(stages=[\n",
    "    RegexTokenizer(\n",
    "        pattern=\"\", inputCol=\"long_hash\", outputCol=\"tokens\", minTokenLength=1\n",
    "    ),\n",
    "    NGram(n=3, inputCol=\"tokens\", outputCol=\"ngrams\"),\n",
    "    HashingTF(inputCol=\"ngrams\", outputCol=\"vectors\"),\n",
    "    MinHashLSH(inputCol=\"vectors\", outputCol=\"lsh\")\n",
    "]).fit(df_left)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b6f3c21-f94d-4580-a288-bd123b602375",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_hashed = model.transform(df_left)\n",
    "query_hashed = model.transform(df_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "340066ec-5a15-407d-af40-1d13c3f2b413",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_hashed = model.transform(df_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b63c2d8e-bc1e-42da-8b1a-77e686b050df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.stages[-1].approxSimilarityJoin(db_hashed, query_hashed, 0.5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9eed50ab-b612-4eee-9be1-c770df5348c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately joining dfA and dfB on Jaccard coefficient distance smaller than 1.5:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(idA='1012', idB='1012', Jaccard=0.0),\n",
       " Row(idA='1002', idB='1002', Jaccard=0.0),\n",
       " Row(idA='1015', idB='1015', Jaccard=0.0),\n",
       " Row(idA='1017', idB='1017', Jaccard=0.0),\n",
       " Row(idA='1020', idB='1020', Jaccard=0.0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "print(\"Approximately joining dfA and dfB on MinHash coefficient distance smaller than 1.5:\")\n",
    "candidates = model.stages[-1].approxSimilarityJoin(db_hashed, query_hashed, 1.5, distCol=\"MinHash\")\\\n",
    "    .select(col(\"datasetA.person_id\").alias(\"idA\"),\n",
    "            col(\"datasetB.person_id\").alias(\"idB\"),\n",
    "            col(\"MinHash\"))\n",
    "\n",
    "candidates.take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1292b97-a55c-4fbf-8965-7985a39f0ad0",
   "metadata": {},
   "source": [
    "## FIND MATCH: trivial match: find id with minimum distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34ab0013-a587-44d1-8bd1-d3174c982ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7c2aefa-459b-45fc-89e4-dac97410101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = Window.partitionBy(\"idB\").orderBy(col(\"MinHash\"))\n",
    "matches = candidates.withColumn(\"row\",row_number().over(w2)) \\\n",
    "  .filter(col(\"row\") == 1).drop(\"row\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c024df-9b8f-4974-a022-a057462e476c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1da8f407-f047-415c-ab84-294d92685968",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.where(col(\"idA\") == col(\"idB\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d561c6a-34c8-48cc-a8c4-86a4b48f4f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.where(col(\"idA\") != col(\"idB\")).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d23a5791-3bb3-44c3-96b2-5d5a77a4c2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_left.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff728cfa-55a4-42e1-9944-f24b5eb58f10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2ebada9-6ab1-40ea-8b4f-bd9e92b9f267",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b31db-b1ed-47fc-ae9e-cf23aec73fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba300e-908f-4568-99f1-9ce61c11a634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2917388-f320-4bdf-8d32-f092cbde1c7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06205f37-4ced-401d-b21d-fe8325f623d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56ee1d77-032e-4c4c-9d9f-5943c425d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have our hashes, we need to decide on a similarity distance and a approximate matching algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba2b76-69e3-44a4-9e01-db795c78a976",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Option 1: home made dice coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "96d0575d-f3b4-4e16-af92-83c51fece150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_digits(bits):\n",
    "    digits = [int(x) for x in list(bits)]\n",
    "    return sum(digits)\n",
    "\n",
    "def dice_coeff(bits1, bits2):\n",
    "    return '10'\n",
    "#     bits = list(zip(bits1, bits2))\n",
    "#     commons = sum([x[0] == x[1] for x in bits])\n",
    "#     return (2 * commons) / (sum_digits(bits1) + sum_digits(bits2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "630a5ddb-283c-4388-832e-cea034f0adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_dice = udf(dice_coeff, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "844136c9-cf09-4901-9653-040a37923e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df1.alias(\"left\")\n",
    "right = df2.alias(\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "494445f7-34f4-4842-a3b9-d660efba7924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(person_id='1000', dx='10'),\n",
       " Row(person_id='1000', dx='10'),\n",
       " Row(person_id='1000', dx='10'),\n",
       " Row(person_id='1000', dx='10'),\n",
       " Row(person_id='1000', dx='10')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left.crossJoin(right).select(df1.person_id,\n",
    "      udf_dice(left.bloom_1, right.bloom_1).alias(\"dx\")).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f9616-e4b1-475c-884c-f282c5fd33a1",
   "metadata": {},
   "source": [
    "self join code for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f3ff381-278c-4853-ae87-34b008a19ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = sparkContext.parallelize(\n",
    "    [(\"a\", 1,2),(\"a\",1,4),(\"b\",5,6),(\"b\",10,2),(\"c\",1,1)]\n",
    "  ).toDF()#\"id\",\"x\",\"y\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c4aacb4-1e62-4529-bca6-f756aaa8dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_1='a', _2=1, _3=2),\n",
       " Row(_1='a', _2=1, _3=4),\n",
       " Row(_1='b', _2=5, _3=6),\n",
       " Row(_1='b', _2=10, _3=2),\n",
       " Row(_1='c', _2=1, _3=1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8db6ea9-0b64-4a62-baa2-0b32a7c4e41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_1='c', dx=0, dy=0),\n",
       " Row(_1='b', dx=0, dy=0),\n",
       " Row(_1='b', dx=0, dy=0),\n",
       " Row(_1='b', dx=0, dy=0),\n",
       " Row(_1='b', dx=0, dy=0),\n",
       " Row(_1='a', dx=0, dy=0),\n",
       " Row(_1='a', dx=0, dy=0),\n",
       " Row(_1='a', dx=0, dy=0),\n",
       " Row(_1='a', dx=0, dy=0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left = df.alias(\"left\")\n",
    "right = df.alias(\"right\")\n",
    "\n",
    "left.join(right,\"_1\").select(df._1,\n",
    "      (left._2-right._2).alias(\"dx\"),\n",
    "      (left._3-right._3).alias(\"dy\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ca01f-8c8c-4f55-94bc-aa27571c9f80",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6233abbe-b6f2-43a6-afae-3d2c7ed04329",
   "metadata": {},
   "source": [
    "## Approx LSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20e605ba-fe05-4141-ad70-3aa13ad77a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataA = [(0, Vectors.dense([1.0, 1.0]),),\n",
    "         (1, Vectors.dense([1.0, -1.0]),),\n",
    "         (2, Vectors.dense([-1.0, -1.0]),),\n",
    "         (3, Vectors.dense([-1.0, 1.0]),)]\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "\n",
    "dataB = [(4, Vectors.dense([1.0, 0.0]),),\n",
    "         (5, Vectors.dense([-1.0, 0.0]),),\n",
    "         (6, Vectors.dense([0.0, 1.0]),),\n",
    "         (7, Vectors.dense([0.0, -1.0]),)]\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n",
    "\n",
    "key = Vectors.dense([1.0, 0.0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "82b7a9ca-8835-4b87-8d48-196e580b8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The hashed dataset where hashed values are stored in the column 'hashes':\n",
      "+---+-----------+--------------------+\n",
      "| id|   features|              hashes|\n",
      "+---+-----------+--------------------+\n",
      "|  0|  [1.0,1.0]|[[6.70322104E8], ...|\n",
      "|  1| [1.0,-1.0]|[[6.70322104E8], ...|\n",
      "|  2|[-1.0,-1.0]|[[6.70322104E8], ...|\n",
      "|  3| [-1.0,1.0]|[[6.70322104E8], ...|\n",
      "+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "brp = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\",\n",
    "                                  numHashTables=3)\n",
    "model = brp.fit(dfA)\n",
    "\n",
    "brp.fit(dfB)\n",
    "\n",
    "# Feature Transformation\n",
    "print(\"The hashed dataset where hashed values are stored in the column 'hashes':\")\n",
    "model.transform(dfA).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "347ba93a-8d1e-423e-ba00-db187a1ad1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|  features|              hashes|\n",
      "+---+----------+--------------------+\n",
      "|  4| [1.0,0.0]|[[6.70322104E8], ...|\n",
      "|  5|[-1.0,0.0]|[[6.70322104E8], ...|\n",
      "|  6| [0.0,1.0]|[[1.055109162E9],...|\n",
      "|  7|[0.0,-1.0]|[[1.055109162E9],...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(dfB).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cc48a7df-0540-4e0f-b5ef-921ee0c2713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately joining dfA and dfB on Jaccard coefficient distance smaller than 1.5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+\n",
      "|idA|idB|Jaccard|\n",
      "+---+---+-------+\n",
      "|  2|  6|    0.5|\n",
      "|  3|  6|    0.5|\n",
      "|  0|  5|    0.5|\n",
      "|  1|  7|    0.5|\n",
      "|  0|  4|    0.5|\n",
      "|  2|  7|    0.5|\n",
      "|  1|  5|    0.5|\n",
      "|  2|  5|    0.5|\n",
      "|  0|  7|    0.5|\n",
      "|  1|  4|    0.5|\n",
      "|  0|  6|    0.5|\n",
      "|  2|  4|    0.5|\n",
      "|  3|  7|    0.5|\n",
      "|  1|  6|    0.5|\n",
      "|  3|  4|    0.5|\n",
      "|  3|  5|    0.5|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the locality sensitive hashes for the input rows, then perform approximate\n",
    "# similarity join.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxSimilarityJoin(transformedA, transformedB, 1.5)`\n",
    "print(\"Approximately joining dfA and dfB on Jaccard coefficient distance smaller than 1.5:\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 1.5, distCol=\"Jaccard\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"Jaccard\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1ce6b313-50ff-4063-8553-68e1e5a5f899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximately searching dfA for 2 nearest neighbors of the key:\n",
      "+---+----------+--------------------+-------+\n",
      "| id|  features|              hashes|distCol|\n",
      "+---+----------+--------------------+-------+\n",
      "|  0| [1.0,1.0]|[[6.70322104E8], ...|    0.5|\n",
      "|  1|[1.0,-1.0]|[[6.70322104E8], ...|    0.5|\n",
      "+---+----------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the locality sensitive hashes for the input rows, then perform approximate nearest\n",
    "# neighbor search.\n",
    "# We could avoid computing hashes by passing in the already-transformed dataset, e.g.\n",
    "# `model.approxNearestNeighbors(transformedA, key, 2)`\n",
    "print(\"Approximately searching dfA for 2 nearest neighbors of the key:\")\n",
    "candidates = model.approxNearestNeighbors(dfA, key, 2)\n",
    "candidates.filter(candidates.distCol > 0.4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf2bfb4-2395-42c2-87de-84b498c4ca9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
