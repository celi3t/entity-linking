{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b93311c-8c62-4668-bb72-a16edbc70cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from hashing.preprocessing import pre_phonetic_encoding, pre_string_clean#, pre_tokenize, pre_sort_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a54fafaa-62c7-4930-9ea3-111b748001af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType, DateType, BinaryType, BooleanType, ByteType, MapType\n",
    "from pyspark.sql.functions import expr, lit\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0413d4b0-e5d4-43e7-9bc3-a702d4bac10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-22 10:12:04 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Entity-Resolution-Hasing').getOrCreate()\n",
    "sparkContext=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea90d077-ce54-470c-ae81-964450c53df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[person_id: string, service_date: string, first_name: string, middle_name: string, last_name: string, dob: string, ssn: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = spark.read.csv(\"../simulated_data/dataset_1.csv\", header=True,\n",
    "    mode=\"DROPMALFORMED\",)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8fc0306-da4d-440a-b910-6015fdcd6a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2 = spark.read.csv(\"../simulated_data/dataset_2.csv\", header=True,\n",
    "#     mode=\"DROPMALFORMED\",)\n",
    "# df2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1388db-ffda-4cd7-a283-10ae69494193",
   "metadata": {},
   "source": [
    "## Parameter Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e75a732-c4ba-4e79-bd3c-5460dc411c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fields': [{'field_origin': 'first_name',\n",
       "   'field_name': 'name_phonetic',\n",
       "   'preprocess_steps': ['drop_chars', 'sort', 'phonetic'],\n",
       "   'matching_type': 'string',\n",
       "   'ngrams': True,\n",
       "   'weight': 0.5},\n",
       "  {'field_origin': 'ssn',\n",
       "   'field_name': 'ssn_clean',\n",
       "   'preprocess_steps': ['drop_chars'],\n",
       "   'matching_type': 'exact',\n",
       "   'ngrams': False,\n",
       "   'weight': 0.5}]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, import set of unique identifiers and fuzzy identifiers that are agreed upon in the matching\n",
    "f = open(\"param_config.json\")\n",
    "fields = json.load(f)\n",
    "# fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde97315-bcc4-4696-a0cc-44700f269064",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing steps \n",
    "Assign each preprocessing step to a udf, then apply to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e010c11-364a-4e54-8587-8d2753728766",
   "metadata": {},
   "outputs": [],
   "source": [
    "## UDFs\n",
    "udf_pre_phonetic_encoding = udf(pre_phonetic_encoding, StringType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c17652a-19fa-476e-99b9-e4b67d283b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(field_configs, data):\n",
    "    print(field_configs)\n",
    "    data = data.withColumn(field_configs['field_name'], data[field_configs['field_origin']])\n",
    "    for step in field_configs['preprocess_steps']:\n",
    "        if step == 'drop_chars':\n",
    "            data = data#.withColumn(field_configs['field']+'_'+step, #udfXYZ)\n",
    "        elif step == 'sort':\n",
    "            data = data\n",
    "        elif step == 'phonetic':\n",
    "            data = data.withColumn(field_configs['field_name'], udf_pre_phonetic_encoding(field_configs['field_name']))\n",
    "        else:\n",
    "            raise ValueError(\"Unknown preprocessing step\")\n",
    "    return data\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "52616af1-c451-4612-98a0-a86d92bc52a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(configs, data):\n",
    "    for config in configs['fields']:\n",
    "        data = preprocess(config, data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4a76caba-1630-405f-8b5e-b90a86e0491b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'field_origin': 'first_name', 'field_name': 'name_phonetic', 'preprocess_steps': ['drop_chars', 'sort', 'phonetic'], 'matching_type': 'string', 'ngrams': True, 'weight': 0.5}\n",
      "{'field_origin': 'ssn', 'field_name': 'ssn_clean', 'preprocess_steps': ['drop_chars'], 'matching_type': 'exact', 'ngrams': False, 'weight': 0.5}\n"
     ]
    }
   ],
   "source": [
    "data_proc = preprocess_all(fields, df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8470b249-471b-4518-a93f-6bcd63175a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b6ed5e3-98ec-46c9-bde7-ddb05a808e3a",
   "metadata": {},
   "source": [
    "## Now to the hashing and linking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb82960-8b75-47b1-be9b-221fd364d34c",
   "metadata": {},
   "source": [
    "### Bloom filter on name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30604f4c-a0b2-41cd-b6de-6efcf5196e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringType"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TO DO: MOVE TO bloom.py\n",
    "import bitarray\n",
    "import mmh3\n",
    "def bloom_hash(item, size, hash_count):\n",
    "    bit_array = bitarray.bitarray(size)\n",
    "    bit_array.setall(0)\n",
    "    for ii in range(hash_count):\n",
    "        index = mmh3.hash(item, ii) % size\n",
    "        bit_array[index] = 1\n",
    "    return bytes(bit_array)\n",
    "\n",
    "bitarray_type = StringType()#ArrayType(ByteType())\n",
    "bitarray_type.needConversion()\n",
    "bitarray_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68fdc03c-9e14-4ade-8fb0-1fb6149e08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_bloom = udf(bloom_hash, bitarray_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a99fda9-b410-4590-85aa-49d1ad71cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloom_filters(configs, data, key = '_bloomhash'):\n",
    "    for field in configs['fields']:\n",
    "        data = data.withColumn(field['field_name'] + key, udf_bloom(field['field_name'], lit(100), lit(5)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf91eb39-d48e-4e07-b2e6-2330c9e515a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc = bloom_filters(fields, data_proc, key = '_bloomhash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a982fa-52ad-4db0-8516-d6d4307aff81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cc0e1f6-02f5-4e3b-bc7d-507b1168e868",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_proc.write.option(\"header\", True).csv(\"hashed_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73582dca-8d59-41eb-8b57-c9c9cbd365b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_proc = df1.withColumn('index_key', \n",
    "#                     sf.concat(sf.col('bloom_1'),sf.lit('_'), sf.col('bloom_2')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2190ea-1bab-4d55-aaeb-a483136526ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1da8f407-f047-415c-ab84-294d92685968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c1ca1cb-2d51-42b7-8376-37f21a0ea866",
   "metadata": {},
   "source": [
    "#### Steps\n",
    "For each bloom filter function:\n",
    "* Map: \"Hash\" entities according to algorithm\n",
    "* Map: Assign block, sum 1s in hash\n",
    "* Join: inner join to compute pairwise distances (Dice coefficient)\n",
    "* Groupby: find min distance\n",
    "* Filter for entities that meet threshold - these are the potential candidates\n",
    "\n",
    "For all entity dataframes with associated candidates\n",
    "* join by entity\n",
    "* Pick candidate (how tho? :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0991f-ddda-47e2-8a31-db18b8e68cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
